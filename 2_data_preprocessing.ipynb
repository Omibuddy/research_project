{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATIONS_ID MESS_DATUM  QN_3    FX    FM  QN_4   RSK  RSKF   SDK  SHK_TAG  \\\n",
      "0         1975 2015-01-01    10  17.8   6.4    10   3.9     6   1.2        0   \n",
      "1         1975 2015-01-02    10  21.9   9.4    10   4.0     8   0.8        0   \n",
      "2         1975 2015-01-03    10  16.3   7.0    10   6.1     6   0.0        0   \n",
      "3         1975 2015-01-04    10  13.0   4.9    10   0.0     0   6.4        0   \n",
      "4         1975 2015-01-05    10   9.9   3.9    10   0.0     6   0.0        0   \n",
      "\n",
      "     NM   VPM      PM   TMK   UPM   TXK   TNK   TGK  eor  \n",
      "0   7.2   6.7  1026.2   3.6  84.0   5.5   2.0   1.1  eor  \n",
      "1   6.5   7.7  1014.6   6.3  81.0   9.2   4.1   2.6  eor  \n",
      "2   5.5   7.0  1014.6   4.5  83.0   5.9   2.4   1.0  eor  \n",
      "3   3.0   6.9  1023.2   4.0  85.0   6.4   2.6   0.2  eor  \n",
      "4   7.8   7.2  1026.4   3.8  90.0   4.4   2.6   2.6  eor  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/omkarsomeshwarkondhalkar/Movies/Notes/PCDL25/Research Project/data/raw/tageswerte_KL_01975_19360101_20241231_hist/produkt_klima_tag_19360101_20241231_01975.txt\",\n",
    "    sep=\";\",\n",
    "    comment=\"#\",\n",
    "    parse_dates=[\"MESS_DATUM\"]\n",
    ")\n",
    "\n",
    "# Filter for recent years: 2015â€“2024\n",
    "df_recent = df[(df['MESS_DATUM'] >= '2015-01-01') & (df['MESS_DATUM'] <= '2024-12-31')].reset_index(drop=True)\n",
    "\n",
    "print(df_recent.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# drop unneeded columns\n",
    "df = df.drop(['STATIONS_ID', 'QN_4', 'RSKF', 'QN_3', 'eor'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MESS_DATUM', '  FX', '  FM', ' RSK', ' SDK', 'SHK_TAG', '  NM', ' VPM', '  PM', ' TMK', ' UPM', ' TXK', ' TNK', ' TGK']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3v/th4wmr295kj9nzlnngytb3100000gn/T/ipykernel_67279/232427099.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32501, 7, 13) (32501,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Strip whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Fill missing values with forward fill\n",
    "df = df.fillna(method='ffill')\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "features = ['FX', 'FM', 'RSK', 'SDK', 'SHK_TAG', 'NM', 'VPM', 'PM', 'TMK', 'UPM', 'TXK', 'TNK', 'TGK']\n",
    "df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "# Create sequence data\n",
    "def create_seq(data, features, target_col, seq_length=7):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data.iloc[i:(i+seq_length)][features].values\n",
    "        y = data.iloc[i+seq_length][target_col]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, y = create_seq(df, features, target_col='TMK', seq_length=7)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (22750, 7, 13) (22750,)\n",
      "Validation set: (4875, 7, 13) (4875,)\n",
      "Test set: (4876, 7, 13) (4876,)\n"
     ]
    }
   ],
   "source": [
    "n_samples = X.shape[0]\n",
    "\n",
    "#calculate split indices\n",
    "train_end = int(0.7 * n_samples)\n",
    "val_end = int(0.85 * n_samples)\n",
    "\n",
    "#chronological data\n",
    "X_train, y_train = X[:train_end], y[:train_end]\n",
    "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
    "X_test, y_test = X[val_end:], y[val_end:]\n",
    "\n",
    "print('Train set:', X_train.shape, y_train.shape)\n",
    "print('Validation set:', X_val.shape, y_val.shape)\n",
    "print('Test set:', X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 22750\n",
      "Validation set size: 4875\n",
      "Test set size: 4876\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#convert numpy arrays to pytorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "#create datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "#create dataloaders\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Train set size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation set size: {len(val_loader.dataset)}\")\n",
    "print(f\"Test set size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
